\chapter{Evaluation and Results}
\label{evaluation}

In this appendix, we show that the XTAG grammar, which uses only
corpus-independent statistical information, can match the performance of
induced probabilistic grammars as well as other rule-based grammars.  Given the
fact that the grammar has not been fine-tuned to the test data, we predict
still better results if it were tailored to the domain of the test data.


XTAG has recently been used to parse Wall Street Journal\footnote{Sentences of
length $\leq$ 15 words.}, IBM manual, and ATIS corpora as a means of evaluating
the coverage and correctness of XTAG parses. For this evaluation, a sentence is
considered to have parsed if the correct parse is among the parses generated by
XTAG. Verifying the presence of the correct parse among the generated parses is
done manually at present by random sampling. Preliminary results without the
use of parse ranking are shown in Table~\ref{results}.  It is worth emphasizing
that the XTAG grammar is truly wide-coverage and has not been fine-tuned to any
particular genre, unlike many other grammars.

\begin{table}[ht]
\begin{center}
\begin{tabular}{|l|c|c|c|} \hline
& \# of & & Av. \# of\\
Corpus & Sentences & \% Parsed & Parses/Sent\\ \hline

WSJ & 18,730 & 41.22 \% & 7.46 \\\hline

IBM Manual & 2040 & 75.42\% & 6.12\\ \hline

ATIS & 524 & 88.35\% & 6.0 \\ \hline 
\end{tabular}
\end{center}

\vspace{0.1in}

\caption{Performance of XTAG on various corpora}
\label{results}
\end{table}

Performance on the WSJ corpus is lower relative to IBM and ATIS due
to the wide-variety of syntactic constructions used. Even grammars
induced on the partially bracketed WSJ corpus have fairly low
performance (e.g. 57.1\% sentence accuracy for \cite{schabes93}).  

\section{Comparison with IBM}

A more detailed experiment to measure the crossing bracket accuracy of the
XTAG-parsed IBM-manual sentences has been performed. In this experiment, XTAG
parses of 1100 IBM-manual sentences have been ranked using certain
heuristics. The ranked parses have been compared\footnote{We used the parseval
program written by Phil Harison (phil@atc.boeing.com).}  against the bracketing
given in the Lancaster Treebank of IBM-manual sentences\footnote{The Treebank
was obtained through Salim Roukos (roukos@watson.ibm.com) at
IBM.}. Table~\ref{ibm-results} shows the results of XTAG obtained in this
experiment, which used the highest ranked parse for each system. It also shows
the results of the latest IBM statistical grammar (\cite{jelineketal94}) on the
same genre of sentences. Only the highest-ranked parse of both systems was used
for this evaluation. Crossing Brackets is the percentage of sentences with no
pairs of brackets crossing the Treebank bracketing (i.e.  (~(~a~b~)~c~) has a
crossing bracket measure of one if compared to (~a~(~b~c~)~)~). Recall is the
ratio of the number of constituents in the XTAG parse to the number of
constituents in the corresponding Treebank sentence.  Precision is the ratio of
the number of correct constituents to the total number of constituents in the
XTAG parse.

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|} \hline 
System & \# of & Crossing Bracket & Recall & Precision \\
& sentences & Accuracy & & \\ \hline
XTAG & 1100 & 81.29\% & 82.34\% & 55.37\% \\ \hline
IBM Statistical & 1100 & 86.20\% & 86.00\% & 85.00\% \\
grammar & &  &  &\\ \hline
\end{tabular}

\vspace{0.1in}

\caption{Performance of XTAG on IBM-manual sentences}
\label{ibm-results} 

\end{table}

As can be seen from Table~\ref{ibm-results}, the precision figure for
the XTAG system is considerably lower than that for IBM. For the
purposes of comparative evaluation against other systems, we had to
use the same crossing-brackets metric though we believe that the
crossing-brackets measure is inadequate for evaluating a grammar like
XTAG. There are two reasons for the inadequacy. First,
the parse generated by XTAG is much richer in its
representation of the internal structure of certain phrases than those
present in manually created treebanks (e.g. IBM: [$_N$ your personal
computer], XTAG: [$_{NP}$ [$_G$ your] [$_N$ [$_N$ personal] [$_N$
computer]]]). This is reflected in the number of constituents per
sentence, shown in the last column of
Table~\ref{const-no}.\footnote{We are aware of the fact that
increasing the number of constituents also increases the recall
percentage. However we believe that this a legitimate gain.}

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|} \hline
System & Sent. & \# of & Av. \# of & Av. \# of \\
& Length & sent & words/sent & Constituents/sent \\ \hline
XTAG & 1-10 & 654 & 7.45 & 22.03  \\ \cline{2-5}
& 1-15 & 978 & 9.13 & 30.56 \\ \hline
IBM Stat. & 1-10 & 447 & 7.50 & 4.60 \\ \cline{2-5}
Grammar & 1-15 & 883 & 10.30 & 6.40 \\ \hline
\end{tabular}
\caption{Constituents in XTAG parse and IBM parse}
\label{const-no}
\end{table}

A second reason for considering the crossing bracket measure
inadequate for evaluating XTAG is that the primary
structure in XTAG is the derivation tree from which the bracketed tree
is derived. Two identical bracketings for a sentence can have
completely different derivation trees (e.g. {\it kick the bucket} as
an idiom vs. a compositional use). A more direct measure of the
performance of XTAG  would evaluate the derivation
structure, which captures the dependencies between words.

\section{Comparison with Alvey}

We also compared XTAG to the Alvey Natural Language Tools (ANLT) Grammar, and
found that the two performed comparably. We parsed the set of LDOCE Noun
Phrases presented in Appendix B of the technical report (\cite{Carroll93})
using XTAG. Table~\ref{Alvey-xtag} summarizes the results of this experiment.
A total of 143 noun phrases were parsed. The NPs which did not have a correct
parse in the top three derivations were considered failures for either
system. The maximum and average number of derivations columns show the highest
and the average number of derivations produced for the NPs that have a correct
derivation in the top three. We show the performance of XTAG both with and
without the tagger since the performance of the POS tagger is significantly
degraded on the NPs because the NPs are usually shorter than the sentences on
which it was trained. It would be interesting to see if the two systems
performed similarly on a wider range of data.

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|c|c|c|}  \hline
% This table will compare the performance of the XTAG with Alvey.
System & \# of & \# parsed & \% parsed & Maximum & Average \\
& NPs &&& derivations & derivations \\ \hline
ANLT Parser & 143 & 127 & 88.81\% & 32 & 4.57 \\ \hline
XTAG Parser with & 143 & 93 & 65.03\% & 28 & 3.45 \\
POS tagger & & & & & \\ \hline
XTAG Parser without & 143 & 120 & 83.91\% & 28 & 4.14\\
POS tagger & & & & & \\ \hline
\end{tabular} \\

\vspace{0.1in}

\caption{Comparison of XTAG and ANLT
Parser}
\label{Alvey-xtag}
\end{table}


%In \cite{Carroll93}, only the LDOCE NPs
%are annotated with the number of derivations; we are interested
%in getting more data annotated with this information, so that we can
%make further comparisons.

\section{Comparison with CLARE}

We have also compared the performance of XTAG against that of the CLARE-2
system (\cite{clare-report92}) on the ATIS corpus. Table~\ref{clare-results}
shows the performance results. The percentage parsed column for both systems
represents the percentage of sentences that produced any parse.  It must be
noted that the performance result shown for CLARE-2 is without any tuning of
the grammar for the ATIS domain. The performance of CLARE-3, a later version of
the CLARE system, is estimated to be 10\% higher than that of the CLARE-2
system.\footnote{When CLARE-3 is tuned to the ATIS domain, performance
increases to 90\%. However XTAG has not been tuned to the ATIS domain.}

\begin{table}[ht]
\centering
\begin{tabular}{|l|c|c|}  \hline
% This table will compare the performance of the XTAG with CLARE-2
System & Mean length & \% parsed \\ \hline
CLARE-2  & 6.53 & 68.50\% \\ \hline
XTAG  & 7.62 & 88.35\% \\ \hline
\end{tabular}
\caption{Performance of CLARE-2 and XTAG on the ATIS corpus}
\label{clare-results}
\end{table}


In an attempt to compare the performance of the two systems on a wider
range of sentences (from similar genres), we provide in
Table~\ref{clare-results1} the performance of CLARE-2 on LOB corpus
and the performance of XTAG on the WSJ corpus. The performance was
measured on sentences of up to 10 words for both systems.
\begin{table}[ht]
\centering
\begin{tabular}{|c|c|c|c|}  \hline
% This table will compare the performance of the XTAG with CLARE-2
System & Corpus & Mean length & \% parsed \\ \hline
CLARE-2 & LOB & 5.95 & 53.40\% \\ \hline
XTAG & WSJ & 6.00 & 55.58\% \\ \hline
\end{tabular}
\caption{Performance of CLARE-2 and XTAG on LOB and WSJ corpus
respectively}
\label{clare-results1}
\end{table}

It can be seen from the above comparisons that XTAG system performs
comparably to CLARE on texts with limited and varied range of sentence
phenomena.
